from ruffus import *

import PipelineERVs as PipelineERVs
import os
import pandas as pd
import numpy as np
import shutil
import ruffus.cmdline as cmdline

PARAMS = PipelineERVs.getParameters("./pipeline.ini")
for PARAM in PARAMS:
    print "%s\t%s\n" % (PARAM, PARAMS[PARAM])

parser = cmdline.get_argparse(description='Pipeline ERVs')

options = parser.parse_args()

logger, logger_mutex = cmdline.setup_logging(__name__,
                                             options.log_file,
                                             options.verbose)


@follows(mkdir("UBLAST_db"))
@transform("%s/*fa" % PARAMS["sequencedir"],
           regex("%s/(.*).fa" % PARAMS["sequencedir"]),
           r'UBLAST_db/\1.udb')
def makeUBLASTDb(infile, outfile):

    '''
    Builds a UBLAST database of each set of query sequences.
    '''

    os.system("%s/usearch -makeudb_ublast -quiet \
               %s -output %s --alpha aa" % (
        PARAMS['path_to_usearch'], infile, outfile))


@follows(mkdir("host_chromosomes"))
@split("%s/%s.fa" % (PARAMS['genome_directory'], PARAMS['genome']),
       r"host_chromosomes/*.fa")
def genomeToChroms(infile, outfiles):
    '''
    Splits the genome into one fasta file for each chromosome.
    If the genome is not assembled into chromosomes, this is specified in
    pipeline.ini.  The pipeline.ini parameter nchroms is then used - the
    contigs or scaffolds are concatenated into nchroms fasta files.  This
    input type is allows much quicker screening with Exonerate.
    '''
    if int(PARAMS['has_chroms']) == 0:
        PipelineERVs.makeChroms(infile, PARAMS['n_chroms'])
    else:
        PipelineERVs.splitChroms(infile)


@transform(genomeToChroms, suffix(".fa"), ".fa.fai")
def indexChroms(infile, outfile):
    '''
    Indexes all chromosomes (or chromosome constructs generated by
    genomeToChroms using Samtools faidx, for fast sequence retrieval later.
    '''
    fasta = infile
    temp = "%s/temp.fa" % PARAMS['working_directory']
    path_to_reformat = PARAMS['path_to_exonerate'].replace(
        "bin/exonerate", "bin/")
    os.system("%sfastareformat %s > %s" % (path_to_reformat,
                                           fasta, temp))
    shutil.move(temp, fasta)
    os.system("%s faidx %s"
              % (PARAMS['path_to_samtools'], infile))


@follows(indexChroms)
@follows(mkdir("raw_exonerate_output"))
@follows(mkdir("parsed_exonerate_output"))
@follows(genomeToChroms)
@transform("%s/*fa" % PARAMS["sequencedir"],
           regex("%s/(.*).fa" % PARAMS["sequencedir"]),
           add_inputs(genomeToChroms),
           (r"parsed_exonerate_output/\1_results.tsv",
            r"parsed_exonerate_output/\1_results.bed"))
def runandParseExonerate(infiles, outfiles):

    '''
    Runs the protein2dna algorithm in the Exonerate software package with
    the host chromosomes as target sequences and fasta files of retroviral
    sequences as queries.  These need to be fasta files named gag.fa, pol.fa
    and env.fa contained in the "sequencedir" parameter from pipeline.ini.
    It is recommended to use the ERV fasta files provided with this tool.
    '''

    fastaFile = infiles[0]
    chromFiles = infiles[1:]
    fastastem = fastaFile.split("/")[-1].replace(".fa", "")
    min_hit_length = int(PARAMS['min_hit_length'])
    cnames = ["query_id", "query_start", "query_end", "query_strand",
              "target_id", "target_start", "target_end",
              "target_strand", "score", "details", "length"]
    bigdf = pd.DataFrame(columns=cnames)
    for chrom in chromFiles:
        chromstem = chrom.split("/")[-1].replace(".fa", "")
        out = "raw_exonerate_output/%s_%s.tsv" % (fastastem, chromstem)

        PipelineERVs.runExonerate(fastaFile, chrom, out,
                                  PARAMS['path_to_exonerate'])

        smalldf = PipelineERVs.filterExonerate(out, min_hit_length)

        bigdf = bigdf.append(smalldf)
    bigdf.to_csv(outfiles[0], sep="\t")
    bedcols = bigdf[['target_id', 'target_start', 'target_end',
                     'target_strand', 'score', 'query_id']]
    bedcols.to_csv(outfiles[1], sep="\t", header=False, index=False)


@transform(runandParseExonerate, suffix("_results.tsv"),
           "_merged.bed")
def mergeOverlaps(infiles, outfile):

    '''
    Uses bedtools merge to merge neighbouring regions containing sequences
    with similarity to a particular ERV gene.
    '''

    bedfile = infiles[1]
    os.system('%s merge -i %s > %s' % (PARAMS['path_to_bedtools'],
                                       bedfile, outfile))


@transform(mergeOverlaps, suffix("_merged.bed"),
           add_inputs(genomeToChroms),
           ".fa")
def makeFasta(infiles, outfile):

    '''
    Makes a FASTA file containing the sequences of the ERV regions generated
    with mergeOverlaps.
    '''

    string = ""
    D = dict()
    with open(infiles[0]) as inf:
        for line in inf:
            line = line.strip().split("\t")
            chrom = line[0]
            if chrom not in D:
                D[chrom] = []
            D[chrom].append("%s:%s-%s" % (line[0], line[1], line[2]))

    for chrom in D:
        string = " ".join(D[chrom])
        os.system('%s faidx host_chromosomes/%s.fa %s >> %s'
                  % (PARAMS['path_to_samtools'], chrom, string, outfile))


@transform(makeFasta, suffix(".fa"), add_inputs(makeUBLASTDb),
           ("_table.tsv", "_alignments.txt", "_filtered.fa"))
def runUBLASTCheck(infiles, outfiles):

    '''
    Uses UBLAST to back check identified ERV sequences against the input
    database, filtering sequences with low homology.  Thresholds can be
    set in pipeline.ini.
    '''

    fasta = infiles[0]
    fastanam = fasta.split("/")[-1].replace(".fa", "")
    for inf in infiles[1:]:
        if inf.split("/")[-1].replace(".udb", "") == fastanam:
            dbnam = inf
    id_thresh = PARAMS['usearch_id']
    min_coverage = PARAMS['usearch_coverage']
    os.system("""%s/usearch -ublast %s \
                 -db %s \
                 -evalue 1 -query_cov %s -id %s\
                 -top_hit_only \
                 -blast6out %s \
                 -quiet \
                 -alnout %s""" % (PARAMS['path_to_usearch'], fasta, dbnam,
                                  min_coverage, id_thresh, outfiles[0],
                                  outfiles[1]))
    L = set([line.strip().split("\t")[0]
             for line in open(outfiles[0]).readlines()])
    PipelineERVs.simpleFasta(L, infiles[0], outfiles[2])


@transform(runUBLASTCheck, suffix(".tsv"),
           add_inputs(PARAMS['path_to_refs']),
           ["_matches.tsv", "_bestmatches.tsv"])
def findBest(infiles, outfiles):

    '''
    Runs exonerate dna2dna to establish which of the input sequences is most
    closely related to each newly identified ERV region.  Query sequences
    are newly identified ERVs and target sequences are known ERVs.
    Sequences which cannot be classified are excluded.
    '''

    gene = infiles[0][0].split("/")[1][0:3]
    fasta = infiles[0][2]
    refs = infiles[1]
    os.system("""
    %s \
    --query %s \
    --target %s \
    --showalignment F \
    --showvulgar F \
    --ryo "%%qi\t%%ti\t%%s\n" \
    --verbose 0 \
    --score 500\
    | sort -n \
    > %s """ % (PARAMS['path_to_exonerate'],
                fasta, refs, outfiles[0]))
    res = pd.read_csv(outfiles[0],
                      sep="\t", header=None, names=['id', 'match', 'score'])
    res['gene'] = res['match'].str.split("_").str.get(-1)
    res = res[res['gene'] == gene]
    res = res.drop('gene', 1)
    rtab = pd.DataFrame(columns=['id', 'match', 'score'])
    ids = np.unique(res['id'].values)
    for id in ids:
        subdf = res[res['id'] == id]
        maxs = subdf[subdf['score'] == np.max(subdf['score'])].values[0]
        rtab.loc[maxs[0]] = maxs
    rtab['gene'] = rtab['match'].str.split("_").str.get(-1)
    rtab['genus'] = rtab['match'].str.split("_").str.get(-2)
    rtab.to_csv(outfiles[1], sep="\t")


@transform(findBest, suffix("_matches.tsv"),
           add_inputs(makeFasta), ["_orfs.tsv", "_orfs.fa"])
def ORFs(infiles, outfiles):
    '''
    Runs EMBOSS getorfs to identify open reading frames in the newly identified
    ERV regions and finds the longest ORF in each region.
    '''
    matches = infiles[0][1]
    fasta = matches.replace("_table_bestmatches.tsv", "_filtered.fa")
    PipelineERVs.getORFS(matches, fasta,
                         outfiles, PARAMS['path_to_emboss'])


@transform(ORFs, suffix("_orfs.tsv"), "_groups.tsv")
def makeGroups(infiles, outfile):

    '''
    Retroviruses in the provided reference database have been grouped
    according to sequence similarity to a set of representative retroviruses
    (using the method used in findBest).  Newly identified ERVs are now
    categorised into these groups according to the matching sequence
    identified using findBest.
    '''

    matches = infiles[0]
    convert = pd.read_csv("%s/convert.tsv"
                          % PARAMS['sequencedir'], sep="\t", index_col=0)
    PipelineERVs.group(matches, convert, outfile)


@follows(mkdir("trees"))
@follows(mkdir("fastas"))
@split(makeGroups, ["fastas/*.fasta", "trees/*.tre"])
def makePhyloFastas(infiles, outfiles):

    '''
    Fasta files, alignments and phylogenetic trees are generated for each of
    the small groups of retroviruses identified in the makeGroups stage.
    An image of each tree is also generated.
    '''

    for filename in infiles:
        try:
            grouptable = pd.read_csv(filename, sep="\t", index_col=0)
        except:
            grouptable = []
        if len(grouptable) != 0:
            allgroups = np.unique(grouptable['group'])
            fasta = filename.replace("_table_groups.tsv", "_filtered.fa")
            PipelineERVs.makePhyloFasta(grouptable, allgroups, fasta,
                                        PARAMS['path_to_phyloseqs'],
                                        PARAMS['path_to_mafft'],
                                        PARAMS['path_to_fasttree'],
                                        PARAMS['sequencedir'])


@transform(makePhyloFastas, suffix(".tre"), ".tre")
def PhyloFastasDone(infile, outfile):

    '''
    Allows ruffus to detect completion of the previous stage.
    '''

    pass


@follows(mkdir("group_lists"))
@follows(mkdir("summary_trees"))
@follows(mkdir("summary_fastas"))
@collate(PhyloFastasDone, regex("trees/(.*)\_(\S+)\_(\S+).tre"),
         r"summary_trees/\2_\3.tre")
def makeRepFastas(infiles, outfile):
    '''
    Using the phylogenies generated in the previous step, monophyletic groups
    containing only newly identified ERVs are identified.  A single sequence
    is chosen to represent each of these groups.
    These sequences are then combined with representative known
    retrovirus sequences and a tree is built represneting each gene and genus
    pair for which new ERVs have been identified.
    '''
    gene = outfile.split("_")[-1].replace(".tre", "")
    wd = PARAMS['working_directory']
    genetab = pd.read_csv(
        "%s/parsed_exonerate_output/%ss_table_groups.tsv"
        % (wd, gene), sep="\t", index_col=0)

    pp = PARAMS['path_to_phyloseqs']
    PipelineERVs.makeRepFastas(infiles, genetab, wd, pp,
                               PARAMS['path_to_mafft'],
                               PARAMS['path_to_fasttree'],
                               PARAMS['sequencedir'],
                               outfile)


@merge(makeGroups, "summary.tsv")
def summarise(infiles, outfile):
    '''
    Builds summary plots showing the number of sequences identified per
    chromosome, ORF length, the number of sequences of each genus
    and which groups the sequences belong to.
    '''
    best = infiles
    gagf = best[1]
    polf = best[2]
    envf = best[0]
    try:
        gag = pd.read_csv(gagf, sep="\t", header=0, index_col=0)
    except:
        gag = []

    try:
        pol = pd.read_csv(polf, sep="\t", header=0, index_col=0)
    except:
        pol = []

    try:
        env = pd.read_csv(envf, sep="\t", header=0, index_col=0)
    except:
        env = []

    PipelineERVs.summary(gag, pol, env, outfile,
                         PARAMS['working_directory'])


if __name__ == '__main__':
    cmdline.run(options)
